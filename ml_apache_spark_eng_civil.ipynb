{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/novo-blog-wordpress/2023/07/853ab090-capa-blog_projeto-orientacao-profissional_engenharia-civil.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong><em><snap style=color:navy> Frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, MinMaxScaler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Claudio Sturaro Martinez Junior\n",
      "\n",
      "findspark: 2.0.1\n",
      "sys      : 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]\n",
      "pyspark  : 3.5.0\n",
      "numpy    : 1.24.3\n",
      "decimal  : 1.70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a \"Claudio Sturaro Martinez Junior\" --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong><em><snap style=color:navy> Preparando o Ambiente Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName='Mini-Project-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Mini-Project-5</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1e5773a99d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong><em><snap style=color:navy> Carregando o Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregar dados  como dataframe do spark\t\n",
    "dados = spark.read.csv(r\"C:\\FCD\\Python_Analise_de_Dados\\ML_Mini_Projeto_5\\dados\\dataset.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conferindo o tipo do dataset\n",
    "type(dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1030"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quantas linhas tem o df\n",
    "dados.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+-----+----------------+---------------+-------------+---+-----+\n",
      "|cement| slag|flyash|water|superplasticizer|coarseaggregate|fineaggregate|age|csMPa|\n",
      "+------+-----+------+-----+----------------+---------------+-------------+---+-----+\n",
      "| 540.0|  0.0|   0.0|162.0|             2.5|         1040.0|        676.0| 28|79.99|\n",
      "| 540.0|  0.0|   0.0|162.0|             2.5|         1055.0|        676.0| 28|61.89|\n",
      "| 332.5|142.5|   0.0|228.0|             0.0|          932.0|        594.0|270|40.27|\n",
      "| 332.5|142.5|   0.0|228.0|             0.0|          932.0|        594.0|365|41.05|\n",
      "| 198.6|132.4|   0.0|192.0|             0.0|          978.4|        825.5|360| 44.3|\n",
      "| 266.0|114.0|   0.0|228.0|             0.0|          932.0|        670.0| 90|47.03|\n",
      "| 380.0| 95.0|   0.0|228.0|             0.0|          932.0|        594.0|365| 43.7|\n",
      "| 380.0| 95.0|   0.0|228.0|             0.0|          932.0|        594.0| 28|36.45|\n",
      "| 266.0|114.0|   0.0|228.0|             0.0|          932.0|        670.0| 28|45.85|\n",
      "| 475.0|  0.0|   0.0|228.0|             0.0|          932.0|        594.0| 28|39.29|\n",
      "| 198.6|132.4|   0.0|192.0|             0.0|          978.4|        825.5| 90|38.07|\n",
      "| 198.6|132.4|   0.0|192.0|             0.0|          978.4|        825.5| 28|28.02|\n",
      "| 427.5| 47.5|   0.0|228.0|             0.0|          932.0|        594.0|270|43.01|\n",
      "| 190.0|190.0|   0.0|228.0|             0.0|          932.0|        670.0| 90|42.33|\n",
      "| 304.0| 76.0|   0.0|228.0|             0.0|          932.0|        670.0| 28|47.81|\n",
      "| 380.0|  0.0|   0.0|228.0|             0.0|          932.0|        670.0| 90|52.91|\n",
      "| 139.6|209.4|   0.0|192.0|             0.0|         1047.0|        806.9| 90|39.36|\n",
      "| 342.0| 38.0|   0.0|228.0|             0.0|          932.0|        670.0|365|56.14|\n",
      "| 380.0| 95.0|   0.0|228.0|             0.0|          932.0|        594.0| 90|40.56|\n",
      "| 475.0|  0.0|   0.0|228.0|             0.0|          932.0|        594.0|180|42.62|\n",
      "+------+-----+------+-----+----------------+---------------+-------------+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# visualizacao dos dados df do spark\n",
    "dados.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>slag</th>\n",
       "      <th>flyash</th>\n",
       "      <th>water</th>\n",
       "      <th>superplasticizer</th>\n",
       "      <th>coarseaggregate</th>\n",
       "      <th>fineaggregate</th>\n",
       "      <th>age</th>\n",
       "      <th>csMPa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>276.4</td>\n",
       "      <td>116.0</td>\n",
       "      <td>90.3</td>\n",
       "      <td>179.6</td>\n",
       "      <td>8.9</td>\n",
       "      <td>870.1</td>\n",
       "      <td>768.3</td>\n",
       "      <td>28</td>\n",
       "      <td>44.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>322.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.6</td>\n",
       "      <td>196.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>817.9</td>\n",
       "      <td>813.4</td>\n",
       "      <td>28</td>\n",
       "      <td>31.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>148.5</td>\n",
       "      <td>139.4</td>\n",
       "      <td>108.6</td>\n",
       "      <td>192.7</td>\n",
       "      <td>6.1</td>\n",
       "      <td>892.4</td>\n",
       "      <td>780.0</td>\n",
       "      <td>28</td>\n",
       "      <td>23.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>159.1</td>\n",
       "      <td>186.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.6</td>\n",
       "      <td>11.3</td>\n",
       "      <td>989.6</td>\n",
       "      <td>788.9</td>\n",
       "      <td>28</td>\n",
       "      <td>32.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>260.9</td>\n",
       "      <td>100.5</td>\n",
       "      <td>78.3</td>\n",
       "      <td>200.6</td>\n",
       "      <td>8.6</td>\n",
       "      <td>864.5</td>\n",
       "      <td>761.5</td>\n",
       "      <td>28</td>\n",
       "      <td>32.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1030 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cement   slag  flyash  water  superplasticizer  coarseaggregate  \\\n",
       "0      540.0    0.0     0.0  162.0               2.5           1040.0   \n",
       "1      540.0    0.0     0.0  162.0               2.5           1055.0   \n",
       "2      332.5  142.5     0.0  228.0               0.0            932.0   \n",
       "3      332.5  142.5     0.0  228.0               0.0            932.0   \n",
       "4      198.6  132.4     0.0  192.0               0.0            978.4   \n",
       "...      ...    ...     ...    ...               ...              ...   \n",
       "1025   276.4  116.0    90.3  179.6               8.9            870.1   \n",
       "1026   322.2    0.0   115.6  196.0              10.4            817.9   \n",
       "1027   148.5  139.4   108.6  192.7               6.1            892.4   \n",
       "1028   159.1  186.7     0.0  175.6              11.3            989.6   \n",
       "1029   260.9  100.5    78.3  200.6               8.6            864.5   \n",
       "\n",
       "      fineaggregate  age  csMPa  \n",
       "0             676.0   28  79.99  \n",
       "1             676.0   28  61.89  \n",
       "2             594.0  270  40.27  \n",
       "3             594.0  365  41.05  \n",
       "4             825.5  360  44.30  \n",
       "...             ...  ...    ...  \n",
       "1025          768.3   28  44.28  \n",
       "1026          813.4   28  31.18  \n",
       "1027          780.0   28  23.70  \n",
       "1028          788.9   28  32.77  \n",
       "1029          761.5   28  32.40  \n",
       "\n",
       "[1030 rows x 9 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualizacao dos dados df pandas\n",
    "dados.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cement: double (nullable = true)\n",
      " |-- slag: double (nullable = true)\n",
      " |-- flyash: double (nullable = true)\n",
      " |-- water: double (nullable = true)\n",
      " |-- superplasticizer: double (nullable = true)\n",
      " |-- coarseaggregate: double (nullable = true)\n",
      " |-- fineaggregate: double (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- csMPa: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# schema\n",
    "dados.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong><em><snap style=color:navy> Modulo de Automacao da Preparacao de Dados\n",
    "\n",
    "O MLlib requer que todas as colunas de entrada do dataframe sejam vetorizadas. Vamos criar um funcao Python que ira automatizar nosso trabalho de preparacao dos dados, incluindo a vetorizacao e todas as tarefas necessarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numero de linhas do dataframe original: 1030\n",
      "numero de linhas do novo dataframe: 1030\n"
     ]
    }
   ],
   "source": [
    "# separamos os dados ausentes (se existirem) e removemos (se existirem)\n",
    "dados_com_linhas_removidas = dados.na.drop()\n",
    "print('numero de linhas do dataframe original:', dados.count())\n",
    "print('numero de linhas do novo dataframe:', dados_com_linhas_removidas.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de preparação dos dados\n",
    "def func_modulo_prep_dados(df,\n",
    "                           variaveis_entrada,\n",
    "                           variavel_saida,\n",
    "                           tratar_outliers = True,\n",
    "                           padronizar_dados = True):\n",
    "\n",
    "    print(\"Iniciando o modulo de preparacao dos dados\")\n",
    "\t# Vamos gerar um novo dataframe, renomeando o argumento que representa a variável de saída.\n",
    "    novo_df = df.withColumnRenamed(variavel_saida, 'label')\n",
    "    \n",
    "\t# Convertemos a variável alvo para o tipo numérico como float (encoding)\n",
    "    if str(novo_df.schema['label'].dataType) != 'IntegerType':\n",
    "        novo_df = novo_df.withColumn(\"label\", novo_df[\"label\"].cast(FloatType()))\n",
    "        \n",
    "    # Listas de controle para as variáveis\n",
    "    variaveis_numericas = []\n",
    "    variaveis_categoricas = []\n",
    "    \n",
    "    # Se tiver variáveis de entrada do tipo string, convertemos para o tipo numérico\n",
    "    for coluna in variaveis_entrada:\n",
    "        \n",
    "        # Verifica se a variável é do tipo string\n",
    "        if str(novo_df.schema[coluna].dataType) == 'StringType':\n",
    "            \n",
    "            # Definimos a variável com um sufixo\n",
    "            novo_nome_coluna = coluna + \"_num\"\n",
    "            \n",
    "            # Adicionamos à lista de variáveis categóricas\n",
    "            variaveis_categoricas.append(novo_nome_coluna)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Se não for variável do tipo string, então é numérica e adicionamos na lista correspondente\n",
    "            variaveis_numericas.append(coluna)\n",
    "            \n",
    "            # Colocamos os dados no dataframe de variáveis indexadas\n",
    "            df_indexed = novo_df\n",
    "            \n",
    "    # Se o dataframe tiver dados do tipo string, aplicamos a indexação\n",
    "    # Verificamos se a lista de variáveis categóricas não está vazia\n",
    "    if len(variaveis_categoricas) != 0: \n",
    "        \n",
    "        # Loop pelas colunas\n",
    "        for coluna in novo_df:\n",
    "            \n",
    "            # Se a variável é do tipo string, criamos, treinamos e aplicamos o indexador\n",
    "            if str(novo_df.schema[coluna].dataType) == 'StringType':\n",
    "                \n",
    "                # Cria o indexador\n",
    "                indexer = StringIndexer(inputCol = coluna, outputCol = coluna + \"_num\") \n",
    "                \n",
    "                # Treina e aplica o indexador\n",
    "                df_indexed = indexer.fit(novo_df).transform(novo_df)\n",
    "    else:\n",
    "        \n",
    "        # Se não temos mais variáveis categóricas, então colocamos os dados no dataframe de variáveis indexadas\n",
    "        df_indexed = novo_df\n",
    "        \n",
    "    # Se for necessário tratar outliers, faremos isso agora\n",
    "    if tratar_outliers == True:\n",
    "        print(\"\\nAplicando o tratamento de outliers...\")\n",
    "        \n",
    "        # Dicionário\n",
    "        d = {}\n",
    "        \n",
    "        # Dicionário de quartis das variáveis do dataframe indexado (somente variáveis numéricas)\n",
    "        for col in variaveis_numericas: \n",
    "            d[col] = df_indexed.approxQuantile(col,[0.01, 0.99], 0.25) \n",
    "        \n",
    "        # Agora aplicamos transformação dependendo da distribuição de cada variável\n",
    "        for col in variaveis_numericas:\n",
    "            \n",
    "            # Extraímos a assimetria dos dados e usamos isso para tratar os outliers\n",
    "            skew = df_indexed.agg(skewness(df_indexed[col])).collect() \n",
    "            skew = skew[0][0]\n",
    "            \n",
    "            # Verificamos a assimetria e então aplicamos:\n",
    "            \n",
    "            # Transformação de log + 1 se a assimetria for positiva\n",
    "            if skew > 1:\n",
    "                indexed = df_indexed.withColumn(col, log(when(df[col] < d[col][0], d[col][0])\\\n",
    "                .when(df_indexed[col] > d[col][1], d[col][1])\\\n",
    "                .otherwise(df_indexed[col] ) + 1).alias(col))\n",
    "                print(\"\\nA variável \" + col + \" foi tratada para assimetria positiva (direita) com skew =\", skew)\n",
    "            \n",
    "            # Transformação exponencial se a assimetria for negativa\n",
    "            elif skew < -1:\n",
    "                indexed = df_indexed.withColumn(col, \\\n",
    "                exp(when(df[col] < d[col][0], d[col][0])\\\n",
    "                .when(df_indexed[col] > d[col][1], d[col][1])\\\n",
    "                .otherwise(df_indexed[col] )).alias(col))\n",
    "                print(\"\\nA variável \" + col + \" foi tratada para assimetria negativa (esquerda) com skew =\", skew)\n",
    "                \n",
    "            # Assimetria entre -1 e 1 não precisamos aplicar transformação aos dados\n",
    "\n",
    "    # Vetorização\n",
    "    \n",
    "    # Lista final de atributos\n",
    "    lista_atributos = variaveis_numericas + variaveis_categoricas\n",
    "    \n",
    "    # Cria o vetorizador para os atributos\n",
    "    vetorizador = VectorAssembler(inputCols = lista_atributos, outputCol = 'features')\n",
    "    \n",
    "    # Aplica o vetorizador ao conjunto de dados\n",
    "    dados_vetorizados = vetorizador.transform(df_indexed).select('features', 'label')\n",
    "    \n",
    "    # Se a flag padronizar_dados está como True, então padronizamos os dados colocando-os na mesma escala\n",
    "    if padronizar_dados == True:\n",
    "        print(\"\\nPadronizando o conjunto de dados para o intervalo de 0 a 1...\")\n",
    "        \n",
    "        # Cria o scaler\n",
    "        scaler = MinMaxScaler(inputCol = \"features\", outputCol = \"scaledFeatures\")\n",
    "\n",
    "        # Calcula o sumário de estatísticas e gera o padronizador\n",
    "        global scalerModel\n",
    "        scalerModel = scaler.fit(dados_vetorizados)\n",
    "\n",
    "        # Padroniza as variáveis para o intervalo [min, max]\n",
    "        dados_padronizados = scalerModel.transform(dados_vetorizados)\n",
    "        \n",
    "        # Gera os dados finais\n",
    "        dados_finais = dados_padronizados.select('label', 'scaledFeatures')\n",
    "        \n",
    "        # Renomeia as colunas (requerido pelo Spark)\n",
    "        dados_finais = dados_finais.withColumnRenamed('scaledFeatures', 'features')\n",
    "        \n",
    "        print(\"\\nProcesso Concluído!\")\n",
    "\n",
    "    # Se a flag está como False, então não padronizamos os dados\n",
    "    else:\n",
    "        print(\"\\nOs dados não serão padronizados pois a flag padronizar_dados está com o valor False.\")\n",
    "        dados_finais = dados_vetorizados\n",
    "    \n",
    "    return dados_finais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora aplicamos o modulo de preparacao dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista de variaveis de entrada (todas menos a target)\n",
    "variaveis_entrada = dados.columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variavel alvo / target\n",
    "variavel_saida = dados.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando o modulo de preparacao dos dados\n",
      "\n",
      "Aplicando o tratamento de outliers...\n",
      "\n",
      "A variável age foi tratada para assimetria positiva (direita) com skew = 3.2644145354168086\n",
      "\n",
      "Padronizando o conjunto de dados para o intervalo de 0 a 1...\n",
      "\n",
      "Processo Concluído!\n"
     ]
    }
   ],
   "source": [
    "# aplica a funcao\n",
    "dados_finais = func_modulo_prep_dados(df=dados,\n",
    "\t\t\t\t\t\t\t\t\t  variaveis_entrada=variaveis_entrada,\n",
    "\t\t\t\t\t\t\t\t\t  variavel_saida=variavel_saida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                                                                                      |\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------------+\n",
      "|79.99|[1.0,0.0,0.0,0.3210862619808307,0.07763975155279502,0.6947674418604651,0.20572002007024587,0.07417582417582418]               |\n",
      "|61.89|[1.0,0.0,0.0,0.3210862619808307,0.07763975155279502,0.7383720930232558,0.20572002007024587,0.07417582417582418]               |\n",
      "|40.27|[0.526255707762557,0.3964941569282137,0.0,0.8482428115015974,0.0,0.3808139534883721,0.0,0.739010989010989]                    |\n",
      "|41.05|[0.526255707762557,0.3964941569282137,0.0,0.8482428115015974,0.0,0.3808139534883721,0.0,1.0]                                  |\n",
      "|44.3 |[0.22054794520547943,0.3683917640511965,0.0,0.560702875399361,0.0,0.5156976744186046,0.58078273958856,0.9862637362637363]     |\n",
      "|47.03|[0.3744292237442922,0.31719532554257096,0.0,0.8482428115015974,0.0,0.3808139534883721,0.19066733567486202,0.24450549450549453]|\n",
      "|43.7 |[0.634703196347032,0.2643294379521425,0.0,0.8482428115015974,0.0,0.3808139534883721,0.0,1.0]                                  |\n",
      "|36.45|[0.634703196347032,0.2643294379521425,0.0,0.8482428115015974,0.0,0.3808139534883721,0.0,0.07417582417582418]                  |\n",
      "|45.85|[0.3744292237442922,0.31719532554257096,0.0,0.8482428115015974,0.0,0.3808139534883721,0.19066733567486202,0.07417582417582418]|\n",
      "|39.29|(8,[0,3,5,7],[0.8515981735159817,0.8482428115015974,0.3808139534883721,0.07417582417582418])                                  |\n",
      "|38.07|[0.22054794520547943,0.3683917640511965,0.0,0.560702875399361,0.0,0.5156976744186046,0.58078273958856,0.24450549450549453]    |\n",
      "|28.02|[0.22054794520547943,0.3683917640511965,0.0,0.560702875399361,0.0,0.5156976744186046,0.58078273958856,0.07417582417582418]    |\n",
      "|43.01|[0.7431506849315068,0.13216471897607124,0.0,0.8482428115015974,0.0,0.3808139534883721,0.0,0.739010989010989]                  |\n",
      "|42.33|[0.2009132420091324,0.528658875904285,0.0,0.8482428115015974,0.0,0.3808139534883721,0.19066733567486202,0.24450549450549453]  |\n",
      "|47.81|[0.4611872146118721,0.211463550361714,0.0,0.8482428115015974,0.0,0.3808139534883721,0.19066733567486202,0.07417582417582418]  |\n",
      "|52.91|[0.634703196347032,0.0,0.0,0.8482428115015974,0.0,0.3808139534883721,0.19066733567486202,0.24450549450549453]                 |\n",
      "|39.36|[0.08584474885844746,0.5826377295492489,0.0,0.560702875399361,0.0,0.7151162790697674,0.53411941796287,0.24450549450549453]    |\n",
      "|56.14|[0.547945205479452,0.105731775180857,0.0,0.8482428115015974,0.0,0.3808139534883721,0.19066733567486202,1.0]                   |\n",
      "|40.56|[0.634703196347032,0.2643294379521425,0.0,0.8482428115015974,0.0,0.3808139534883721,0.0,0.24450549450549453]                  |\n",
      "|42.62|(8,[0,3,5,7],[0.8515981735159817,0.8482428115015974,0.3808139534883721,0.4917582417582418])                                   |\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# visualiza os dados\n",
    "dados_finais.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong><em><snap style=color:navy> Verificando a Correlacao\n",
    "\n",
    "Vamos nos certificar que nao temos multicolinearidade antes de prosseguirmos.\n",
    "Lembre-se das seguintes diretrizes para o Coeficiente de Correlacao de Pearson:\n",
    "\n",
    "* .00 - .19 (correlacao muito franca)\n",
    "* .20 - .39 (correlacao fraca)\n",
    "* .40 - .59 (correlacao moderada)\n",
    "* .60 - .79 (correlacao forte)\n",
    "* .80 - 1.0 (correlacao muito forte)\n",
    "\n",
    "\n",
    "<img src=\"https://dhg1h5j42swfq.cloudfront.net/2021/12/19151647/image-411.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrai a correlacao\n",
    "coeficientes_corr = Correlation.corr(dados_finais, 'features', 'pearson').collect()[0][0]\n",
    "array_corr = coeficientes_corr.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.27521591, -0.39746734, -0.08158675,  0.09238617,\n",
       "        -0.10934899, -0.22271785,  0.08194602],\n",
       "       [-0.27521591,  1.        , -0.3235799 ,  0.10725203,  0.04327042,\n",
       "        -0.28399861, -0.28160267, -0.04424602],\n",
       "       [-0.39746734, -0.3235799 ,  1.        , -0.25698402,  0.37750315,\n",
       "        -0.00996083,  0.07910849, -0.15437052],\n",
       "       [-0.08158675,  0.10725203, -0.25698402,  1.        , -0.65753291,\n",
       "        -0.1822936 , -0.45066117,  0.27761822],\n",
       "       [ 0.09238617,  0.04327042,  0.37750315, -0.65753291,  1.        ,\n",
       "        -0.26599915,  0.22269123, -0.19270003],\n",
       "       [-0.10934899, -0.28399861, -0.00996083, -0.1822936 , -0.26599915,\n",
       "         1.        , -0.17848096, -0.00301588],\n",
       "       [-0.22271785, -0.28160267,  0.07910849, -0.45066117,  0.22269123,\n",
       "        -0.17848096,  1.        , -0.1560947 ],\n",
       "       [ 0.08194602, -0.04424602, -0.15437052,  0.27761822, -0.19270003,\n",
       "        -0.00301588, -0.1560947 ,  1.        ]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08194602387182176\n",
      "-0.044246019304454175\n",
      "-0.15437051606792915\n",
      "0.27761822152100296\n",
      "-0.19270002804347258\n",
      "-0.0030158803467436645\n",
      "-0.15609470264758615\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# lista de correlacao entre os atributos e a variavel alvo\n",
    "for item in array_corr:\n",
    "\tprint(item[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong><em><snap style=color:navy> Divisao de Dados de Treino e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_treino, dados_teste = dados_finais.randomSplit([.7, .3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "724"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados_treino.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados_teste.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong><em><snap style=color:navy> Modulo de AutoML (Automated Machine Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Módulo de Machine Learning\n",
    "def func_modulo_ml(algoritmo_regressao):\n",
    "\n",
    "    # Função para obter o tipo do algoritmo de regressão e criar a instância do objeto\n",
    "    # Usaremos isso para automatizar nosso processo\n",
    "    def func_tipo_algo(algo_regressao):\n",
    "        algoritmo = algo_regressao\n",
    "        tipo_algo = type(algoritmo).__name__\n",
    "        return tipo_algo\n",
    "    \n",
    "    # Aplica a função anterior\n",
    "    tipo_algo = func_tipo_algo(algoritmo_regressao)\n",
    "\n",
    "    # Se o algoritmo for Regressão Linear, entramos neste bloco if\n",
    "    if tipo_algo == \"LinearRegression\":\n",
    "        \n",
    "        # Treinamos a primeira versão do modelo sem validação cruzada\n",
    "        modelo = regressor.fit(dados_treino)\n",
    "        \n",
    "        # Métricas do modelo\n",
    "        print('\\033[1m' + \"Modelo de Regressão Linear Sem Validação Cruzada:\" + '\\033[0m')\n",
    "        print(\"\")\n",
    "        \n",
    "        # Avalia o modelo com dados de teste\n",
    "        resultado_teste = modelo.evaluate(dados_teste)\n",
    "\n",
    "        # Imprime as métricas de erro do modelo com dados de teste\n",
    "        print(\"RMSE em Teste: {}\".format(resultado_teste.rootMeanSquaredError))\n",
    "        print(\"Coeficiente R2 em Teste: {}\".format(resultado_teste.r2))\n",
    "        print(\"\")\n",
    "        \n",
    "        # Agora vamos criar a segunda versão do modelo com mesmo algoritmo, mas usando validação cruzada\n",
    "        \n",
    "        # Prepara o grid de hiperparâmetros\n",
    "        paramGrid = (ParamGridBuilder().addGrid(regressor.regParam, [0.1, 0.01]).build())\n",
    "        \n",
    "        # Cria os avaliadores\n",
    "        eval_rmse = RegressionEvaluator(metricName = \"rmse\")\n",
    "        eval_r2 = RegressionEvaluator(metricName = \"r2\")\n",
    "        \n",
    "        # Cria o Cross Validator\n",
    "        crossval = CrossValidator(estimator = regressor,\n",
    "                                  estimatorParamMaps = paramGrid,\n",
    "                                  evaluator = eval_rmse,\n",
    "                                  numFolds = 3) \n",
    "        \n",
    "        print('\\033[1m' + \"Modelo de Regressão Linear Com Validação Cruzada:\" + '\\033[0m')\n",
    "        print(\"\")\n",
    "        \n",
    "        # Treina o modelo com validação cruzada\n",
    "        modelo = crossval.fit(dados_treino)\n",
    "        \n",
    "        # Salva o melhor modelo da versão 2\n",
    "        global LR_BestModel \n",
    "        LR_BestModel = modelo.bestModel\n",
    "                \n",
    "        # Previsões com dados de teste\n",
    "        previsoes = LR_BestModel.transform(dados_teste)\n",
    "        \n",
    "        # Avaliação do melhor modelo\n",
    "        resultado_teste_rmse = eval_rmse.evaluate(previsoes)\n",
    "        print('RMSE em Teste:', resultado_teste_rmse)\n",
    "        \n",
    "        resultado_teste_r2 = eval_r2.evaluate(previsoes)\n",
    "        print('Coeficiente R2 em Teste:', resultado_teste_r2)\n",
    "        print(\"\")\n",
    "    \n",
    "        # Lista de colunas para colocar no dataframe de resumo\n",
    "        columns = ['Regressor', 'Resultado_RMSE', 'Resultado_R2']\n",
    "        \n",
    "        # Formata os resultados e cria o dataframe\n",
    "        \n",
    "        # Formata as métricas e nome do algoritmo\n",
    "        rmse_str = [str(resultado_teste_rmse)] \n",
    "        r2_str = [str(resultado_teste_r2)] \n",
    "        tipo_algo = [tipo_algo] \n",
    "        \n",
    "        # Cria o dataframne\n",
    "        df_resultado = spark.createDataFrame(zip(tipo_algo, rmse_str, r2_str), schema = columns)\n",
    "        \n",
    "        # Grava os resultados no dataframe\n",
    "        df_resultado = df_resultado.withColumn('Resultado_RMSE', df_resultado.Resultado_RMSE.substr(0, 5))\n",
    "        df_resultado = df_resultado.withColumn('Resultado_R2', df_resultado.Resultado_R2.substr(0, 5))\n",
    "        \n",
    "        return df_resultado\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # Verificamos se o algoritmo é o Decision Tree e criamos o grid de hiperparâmetros\n",
    "        if tipo_algo in(\"DecisionTreeRegressor\"):\n",
    "            paramGrid = (ParamGridBuilder().addGrid(regressor.maxBins, [10, 20, 40]).build())\n",
    "\n",
    "        # Verificamos se o algoritmo é o Random Forest e criamos o grid de hiperparâmetros\n",
    "        if tipo_algo in(\"RandomForestRegressor\"):\n",
    "            paramGrid = (ParamGridBuilder().addGrid(regressor.numTrees, [5, 20]).build())\n",
    "\n",
    "        # Verificamos se o algoritmo é o GBT e criamos o grid de hiperparâmetros\n",
    "        if tipo_algo in(\"GBTRegressor\"):\n",
    "            paramGrid = (ParamGridBuilder() \\\n",
    "                         .addGrid(regressor.maxBins, [10, 20]) \\\n",
    "                         .addGrid(regressor.maxIter, [10, 15])\n",
    "                         .build())\n",
    "            \n",
    "        # Verificamos se o algoritmo é o Isotonic \n",
    "        if tipo_algo in(\"IsotonicRegression\"):\n",
    "            paramGrid = (ParamGridBuilder().addGrid(regressor.isotonic, [True, False]).build())\n",
    "\n",
    "        # Cria os avaliadores\n",
    "        eval_rmse = RegressionEvaluator(metricName = \"rmse\")\n",
    "        eval_r2 = RegressionEvaluator(metricName = \"r2\")\n",
    "        \n",
    "        # Prepara o Cross Validator\n",
    "        crossval = CrossValidator(estimator = regressor,\n",
    "                                  estimatorParamMaps = paramGrid,\n",
    "                                  evaluator = eval_rmse,\n",
    "                                  numFolds = 3) \n",
    "        \n",
    "        # Treina o modelo usando validação cruzada\n",
    "        modelo = crossval.fit(dados_treino)\n",
    "        \n",
    "        # Extrai o melhor modelo\n",
    "        BestModel = modelo.bestModel\n",
    "\n",
    "        # Resumo de cada modelo\n",
    "        \n",
    "        # Métricas do modelo\n",
    "        if tipo_algo in(\"DecisionTreeRegressor\"):\n",
    "            \n",
    "            # Variável global\n",
    "            global DT_BestModel \n",
    "            DT_BestModel = modelo.bestModel\n",
    "            \n",
    "            # Previsões com dados de teste\n",
    "            previsoes_DT = DT_BestModel.transform(dados_teste)\n",
    "            \n",
    "            print('\\033[1m' + \"Modelo Decision Tree Com Validação Cruzada:\" + '\\033[0m')\n",
    "            print(\" \")\n",
    "            \n",
    "            # Avaliação do modelo\n",
    "            resultado_teste_rmse = eval_rmse.evaluate(previsoes_DT)\n",
    "            print('RMSE em Teste:', resultado_teste_rmse)\n",
    "        \n",
    "            resultado_teste_r2 = eval_r2.evaluate(previsoes_DT)\n",
    "            print('Coeficiente R2 em Teste:', resultado_teste_r2)\n",
    "            print(\"\")\n",
    "        \n",
    "        # Métricas do modelo\n",
    "        if tipo_algo in(\"RandomForestRegressor\"):\n",
    "            \n",
    "            # Variável global\n",
    "            global RF_BestModel \n",
    "            RF_BestModel = modelo.bestModel\n",
    "            \n",
    "            # Previsões com dados de teste\n",
    "            previsoes_RF = RF_BestModel.transform(dados_teste)\n",
    "            \n",
    "            print('\\033[1m' + \"Modelo RandomForest Com Validação Cruzada:\" + '\\033[0m')\n",
    "            print(\" \")\n",
    "            \n",
    "            # Avaliação do modelo\n",
    "            resultado_teste_rmse = eval_rmse.evaluate(previsoes_RF)\n",
    "            print('RMSE em Teste:', resultado_teste_rmse)\n",
    "        \n",
    "            resultado_teste_r2 = eval_r2.evaluate(previsoes_RF)\n",
    "            print('Coeficiente R2 em Teste:', resultado_teste_r2)\n",
    "            print(\"\")\n",
    "        \n",
    "        # Métricas do modelo\n",
    "        if tipo_algo in(\"GBTRegressor\"):\n",
    "\n",
    "            # Variável global\n",
    "            global GBT_BestModel \n",
    "            GBT_BestModel = modelo.bestModel\n",
    "            \n",
    "            # Previsões com dados de teste\n",
    "            previsoes_GBT = GBT_BestModel.transform(dados_teste)\n",
    "            \n",
    "            print('\\033[1m' + \"Modelo Gradient-Boosted Tree (GBT) Com Validação Cruzada:\" + '\\033[0m')\n",
    "            print(\" \")\n",
    "            \n",
    "            # Avaliação do modelo\n",
    "            resultado_teste_rmse = eval_rmse.evaluate(previsoes_GBT)\n",
    "            print('RMSE em Teste:', resultado_teste_rmse)\n",
    "        \n",
    "            resultado_teste_r2 = eval_r2.evaluate(previsoes_GBT)\n",
    "            print('Coeficiente R2 em Teste:', resultado_teste_r2)\n",
    "            print(\"\")\n",
    "            \n",
    "        # Métricas do modelo\n",
    "        if tipo_algo in(\"IsotonicRegression\"):\n",
    "\n",
    "            # Variável global\n",
    "            global ISO_BestModel \n",
    "            ISO_BestModel = modelo.bestModel\n",
    "            \n",
    "            # Previsões com dados de teste\n",
    "            previsoes_ISO = ISO_BestModel.transform(dados_teste)\n",
    "            \n",
    "            print('\\033[1m' + \"Modelo Isotonic Com Validação Cruzada:\" + '\\033[0m')\n",
    "            print(\" \")\n",
    "            \n",
    "            # Avaliação do modelo\n",
    "            resultado_teste_rmse = eval_rmse.evaluate(previsoes_ISO)\n",
    "            print('RMSE em Teste:', resultado_teste_rmse)\n",
    "        \n",
    "            resultado_teste_r2 = eval_r2.evaluate(previsoes_ISO)\n",
    "            print('Coeficiente R2 em Teste:', resultado_teste_r2)\n",
    "            print(\"\")\n",
    "                    \n",
    "        # Lista de colunas para colocar no dataframe de resumo\n",
    "        columns = ['Regressor', 'Resultado_RMSE', 'Resultado_R2']\n",
    "        \n",
    "        # Faz previsões com dados de teste\n",
    "        previsoes = modelo.transform(dados_teste)\n",
    "        \n",
    "        # Avalia o modelo para gravar o resultado\n",
    "        eval_rmse = RegressionEvaluator(metricName = \"rmse\")\n",
    "        rmse = eval_rmse.evaluate(previsoes)\n",
    "        rmse_str = [str(rmse)]\n",
    "        \n",
    "        eval_r2 = RegressionEvaluator(metricName = \"r2\")\n",
    "        r2 = eval_r2.evaluate(previsoes)\n",
    "        r2_str = [str(r2)]\n",
    "         \n",
    "        tipo_algo = [tipo_algo] \n",
    "        \n",
    "        # Cria o dataframe\n",
    "        df_resultado = spark.createDataFrame(zip(tipo_algo, rmse_str, r2_str), schema = columns)\n",
    "        \n",
    "        # Grava o resultado no dataframe\n",
    "        df_resultado = df_resultado.withColumn('Resultado_RMSE', df_resultado.Resultado_RMSE.substr(0, 5))\n",
    "        df_resultado = df_resultado.withColumn('Resultado_R2', df_resultado.Resultado_R2.substr(0, 5))\n",
    "        \n",
    "        return df_resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de algoritmos\n",
    "regressores = [LinearRegression(),\n",
    "               DecisionTreeRegressor(),\n",
    "               RandomForestRegressor(),\n",
    "               GBTRegressor(),\n",
    "               IsotonicRegression()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de colunas e valores\n",
    "colunas = ['Regressor', 'Resultado_RMSE', 'Resultado_R2']\n",
    "valores = [(\"N/A\", \"N/A\", \"N/A\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara a tabela de resumo\n",
    "df_resultados_treinamento = spark.createDataFrame(valores, colunas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mModelo de Regressão Linear Sem Validação Cruzada:\u001b[0m\n",
      "\n",
      "RMSE em Teste: 10.524027720507188\n",
      "Coeficiente R2 em Teste: 0.5563120597984652\n",
      "\n",
      "\u001b[1mModelo de Regressão Linear Com Validação Cruzada:\u001b[0m\n",
      "\n",
      "RMSE em Teste: 10.52392326872791\n",
      "Coeficiente R2 em Teste: 0.5563208670281772\n",
      "\n",
      "\u001b[1mModelo Decision Tree Com Validação Cruzada:\u001b[0m\n",
      " \n",
      "RMSE em Teste: 8.069473343362006\n",
      "Coeficiente R2 em Teste: 0.7391420720525737\n",
      "\n",
      "\u001b[1mModelo RandomForest Com Validação Cruzada:\u001b[0m\n",
      " \n",
      "RMSE em Teste: 6.748382068514965\n",
      "Coeficiente R2 em Teste: 0.8175629743143521\n",
      "\n",
      "\u001b[1mModelo Gradient-Boosted Tree (GBT) Com Validação Cruzada:\u001b[0m\n",
      " \n",
      "RMSE em Teste: 6.18694330354034\n",
      "Coeficiente R2 em Teste: 0.8466563028036085\n",
      "\n",
      "\u001b[1mModelo Isotonic Com Validação Cruzada:\u001b[0m\n",
      " \n",
      "RMSE em Teste: 13.507357139323611\n",
      "Coeficiente R2 em Teste: 0.2691059548967488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop de treinamento\n",
    "for regressor in regressores:\n",
    "    \n",
    "    # Para cada regressor obtém o resultado\n",
    "    resultado_modelo = func_modulo_ml(regressor)\n",
    "    \n",
    "    # Grava os resultados\n",
    "    df_resultados_treinamento = df_resultados_treinamento.union(resultado_modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retorna as linhas diferentes de N/A\n",
    "df_resultados_treinamento = df_resultados_treinamento.where(\"Regressor!='N/A'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------+------------+\n",
      "|Regressor            |Resultado_RMSE|Resultado_R2|\n",
      "+---------------------+--------------+------------+\n",
      "|LinearRegression     |10.52         |0.556       |\n",
      "|DecisionTreeRegressor|8.069         |0.739       |\n",
      "|RandomForestRegressor|6.748         |0.817       |\n",
      "|GBTRegressor         |6.186         |0.846       |\n",
      "|IsotonicRegression   |13.50         |0.269       |\n",
      "+---------------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imprime\n",
    "df_resultados_treinamento.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusao:\n",
    "O modelo GBTRegressor apresentou a melhor performance, pois o erro RMSE foi o menor (bom) e a precisao do R2 (compreensao da variabilidade dos dados) foi a maior (bom)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
